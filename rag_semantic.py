"""
rag_semantic.py - –ù–ê–°–¢–û–Ø–©–ò–ô –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""

print("=" * 70)
print("ü§ñ RAG –° –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ú –ü–û–ò–°–ö–û–ú")
print("=" * 70)

import pickle
import numpy as np
from sentence_transformers import SentenceTransformer, util
import warnings

warnings.filterwarnings('ignore')

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
try:
    with open('rag_data_step2.pkl', 'rb') as f:
        rag_data = pickle.load(f)

    print("‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    chunks = rag_data["chunks"]
    chunk_info = rag_data["chunk_info"]
    chunk_embeddings = rag_data["embeddings"]

    print(f"üìö –í –±–∞–∑–µ: {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    print(f"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {chunk_embeddings.shape[1]}")

except FileNotFoundError:
    print("‚ùå –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
    print("  python 1_data_preparation.py")
    print("  python 2_embedding_visual.py")
    exit()

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –í–û–ü–†–û–°–û–í
# –¢–ê –ñ–ï –º–æ–¥–µ–ª—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤!
print("\nüß† –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞!")


def semantic_search(question, top_k=3):
    """
    –ù–ê–°–¢–û–Ø–©–ò–ô —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫:
    1. –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –≤–æ–ø—Ä–æ—Å
    2. –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    3. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """

    print(f"\nüîç –í–æ–ø—Ä–æ—Å: '{question}'")
    print("   ‚Üì")
    print("   üìä –ü—Ä–µ–æ–±—Ä–∞–∑—É—é –≤ –≤–µ–∫—Ç–æ—Ä...")

    # 1. –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –í–û–ü–†–û–°–ê
    question_embedding = model.encode(question, convert_to_tensor=True)

    # 2. –ü–û–ò–°–ö –ë–õ–ò–ñ–ê–ô–®–ò–• –í–ï–ö–¢–û–†–û–í
    print("   üìä –ò—â—É –ø–æ—Ö–æ–∂–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã...")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    cosine_scores = util.cos_sim(question_embedding, chunk_embeddings)[0]

    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    top_results = np.argsort(cosine_scores.numpy())[::-1][:top_k]

    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(top_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

    # 3. –§–û–†–ú–ò–†–£–ï–ú –†–ï–ó–£–õ–¨–¢–ê–¢–´
    results = []
    for rank, idx in enumerate(top_results, 1):
        similarity = cosine_scores[idx].item()

        print(f"\n   #{rank} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.3f}):")
        print(f"   üìÑ {chunks[idx][:100]}...")

        results.append({
            "rank": rank,
            "chunk_index": idx,
            "similarity": similarity,
            "text": chunks[idx],
            "doc_id": chunk_info[idx]["doc_id"]
        })

    return results, question_embedding


def generate_answer_from_context(question, search_results):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    (–í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ ChatGPT)
    """

    print("\nüß† –§–æ—Ä–º–∏—Ä—É—é –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ...")

    # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    context_parts = []
    for result in search_results:
        context_parts.append(f"[–§—Ä–∞–≥–º–µ–Ω—Ç #{result['rank']} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {result['similarity']:.3f})]:")
        context_parts.append(result['text'])
        context_parts.append("")

    context = "\n".join(context_parts[:500])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º, —á—Ç–æ –Ω–∞—à–ª–∏
    print("\nüìã –ê–ù–ê–õ–ò–ó –ù–ê–ô–î–ï–ù–ù–û–ì–û:")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É –≤–æ–ø—Ä–æ—Å–∞
    question_lower = question.lower()

    # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    if any(word in question_lower for word in ["–º–∞—à–∏–Ω–Ω", "ml", "machine learning", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]):
        topic = "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
    elif any(word in question_lower for word in ["–≥–ª—É–±–æ–∫", "deep learning", "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–Ω–µ–π—Ä–æ–Ω–Ω–∞—è"]):
        topic = "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
    elif any(word in question_lower for word in ["—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä", "gpt", "bert", "llm", "—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å"]):
        topic = "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã"
    elif any(word in question_lower for word in ["rag", "retrieval", "–ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è"]):
        topic = "RAG"
    else:
        topic = "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ò–ò"

    print(f"   –¢–µ–º–∞ –≤–æ–ø—Ä–æ—Å–∞: {topic}")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    print("\nü§ñ –û–¢–í–ï–¢:")
    print("-" * 50)

    if search_results:
        # –ë–µ—Ä–µ–º —Å–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏ –¥–æ–ø–æ–ª–Ω—è–µ–º
        best_result = search_results[0]
        best_text = best_result["text"]

        # –£–ø—Ä–æ—â–∞–µ–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ–º
        sentences = best_text.replace('\n', ' ').split('. ')
        if sentences:
            # –ë–µ—Ä–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            key_sentences = []
            for sentence in sentences:
                if len(sentence) > 20:  # –ù–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
                    if topic.lower() in sentence.lower():
                        key_sentences.append(sentence)

            if not key_sentences:
                key_sentences = sentences[:3]  # –ü–µ—Ä–≤—ã–µ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

            answer = f"–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ {topic}:\n\n"
            answer += ". ".join(key_sentences[:3]) + ".\n\n"

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –∏–∑ –¥—Ä—É–≥–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            if len(search_results) > 1:
                second_result = search_results[1]
                if second_result["similarity"] > 0.6:
                    second_sentences = second_result["text"].replace('\n', ' ').split('. ')
                    if second_sentences:
                        answer += f"–¢–∞–∫–∂–µ:\n{second_sentences[0]}.\n"

            answer += f"\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã #{', #'.join(str(r['rank']) for r in search_results[:2])}"

            print(answer)
        else:
            print("–ù–∞—à–µ–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–æ –Ω–µ —Å–º–æ–≥ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑–Ω—ã–π –æ—Ç–≤–µ—Ç.")
    else:
        print("–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.")

    print("-" * 50)

    return context


# –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –ü–û–ò–°–ö–ê
print("\n" + "=" * 70)
print("üß™ –¢–ï–°–¢–ò–†–£–ï–ú –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö")
print("=" * 70)

test_questions = [
    "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
    "–û–±—ä—è—Å–Ω–∏ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
    "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã?",
    "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç RAG —Å–∏—Å—Ç–µ–º–∞?",
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã:
    "–ù–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞",
    "–ú–æ–¥–µ–ª–∏ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è",
    "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
    "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö"
]

print("\nüìã –¢–ï–°–¢–û–í–´–ï –í–û–ü–†–û–°–´:")
for i, q in enumerate(test_questions, 1):
    print(f"{i}. {q}")

print("\nüöÄ –ó–∞–ø—É—Å–∫–∞—é —Ç–µ—Å—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞...")

for i, question in enumerate(test_questions[:4], 1):  # –ü–µ—Ä–≤—ã–µ 4 –≤–æ–ø—Ä–æ—Å–∞
    print(f"\n{'=' * 70}")
    print(f"–¢–ï–°–¢ {i}: '{question}'")
    print('=' * 70)

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    results, question_embedding = semantic_search(question)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    context = generate_answer_from_context(question, results)

print("\n" + "=" * 70)
print("üéÆ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú –° –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ú –ü–û–ò–°–ö–û–ú")
print("=" * 70)

print("""
–¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –õ–Æ–ë–´–ú–ò —Å–ª–æ–≤–∞–º–∏!
–°–∏—Å—Ç–µ–º–∞ –∏—â–µ—Ç –ø–æ –°–ú–´–°–õ–£, –∞ –Ω–µ –ø–æ —Ç–æ—á–Ω—ã–º —Å–ª–æ–≤–∞–º.

–ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ç–µ–ø–µ—Ä—å –¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å:
‚Ä¢ "–ù–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞" ‚Üí –Ω–∞–π–¥–µ—Ç –ø—Ä–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã
‚Ä¢ "–ú–æ–¥–µ–ª–∏ —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º" ‚Üí –Ω–∞–π–¥–µ—Ç –ø—Ä–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã  
‚Ä¢ "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö" ‚Üí –Ω–∞–π–¥–µ—Ç –ø—Ä–æ supervised learning
‚Ä¢ "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤" ‚Üí –Ω–∞–π–¥–µ—Ç –ø—Ä–æ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ

–ö–æ–º–∞–Ω–¥—ã:
‚Ä¢ '–≤—ã—Ö–æ–¥' - –∑–∞–≤–µ—Ä—à–∏—Ç—å
‚Ä¢ '—Ç–µ—Å—Ç' - –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã
‚Ä¢ '—Å—Ç–∞—Ç' - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã
""")

# –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú
while True:
    print("\n" + "=" * 50)
    question = input("\n‚ùì –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()

    if question.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', 'q']:
        print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
        break

    if question.lower() in ['—Ç–µ—Å—Ç', 'test']:
        print("\nüß™ –ó–∞–ø—É—Å–∫–∞—é –ø–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        for q in test_questions:
            print(f"\n--- {q} ---")
            results, _ = semantic_search(q, top_k=2)
        continue

    if question.lower() in ['—Å—Ç–∞—Ç', 'stats', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞']:
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:")
        print(f"‚Ä¢ –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(chunks)}")
        print(f"‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {chunk_embeddings.shape[1]}")
        print(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞: {np.mean([len(c) for c in chunks]):.0f} —Å–∏–º–≤–æ–ª–æ–≤")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–µ–º–∞–º
        topics = {"ML": 0, "DL": 0, "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã": 0, "RAG": 0, "–î—Ä—É–≥–æ–µ": 0}
        for chunk in chunks:
            chunk_lower = chunk.lower()
            if any(word in chunk_lower for word in ["–º–∞—à–∏–Ω–Ω", "ml", "machine"]):
                topics["ML"] += 1
            elif any(word in chunk_lower for word in ["–≥–ª—É–±–æ–∫", "deep", "–Ω–µ–π—Ä–æ–Ω"]):
                topics["DL"] += 1
            elif any(word in chunk_lower for word in ["—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä", "gpt", "bert"]):
                topics["–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã"] += 1
            elif any(word in chunk_lower for word in ["rag", "retrieval"]):
                topics["RAG"] += 1
            else:
                topics["–î—Ä—É–≥–æ–µ"] += 1

        print("\nüìö –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–µ–º–∞–º:")
        for topic, count in topics.items():
            if count > 0:
                print(f"  {topic}: {count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        continue

    if not question:
        continue

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞
    try:
        results, _ = semantic_search(question)
        context = generate_answer_from_context(question, results)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –≤–æ–ø—Ä–æ—Å")

print("\n" + "=" * 70)
print("‚úÖ –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö –†–ï–ê–õ–ò–ó–û–í–ê–ù!")
print("=" * 70)
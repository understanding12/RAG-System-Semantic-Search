"""
3_search_engine.py - –ü–û–ò–°–ö–û–í–ê–Ø –°–ò–°–¢–ï–ú–ê –ü–û –°–ú–´–°–õ–£
–ë–µ–∑ –ª–∏—à–Ω–∏—Ö –∏–º–ø–æ—Ä—Ç–æ–≤ PyTorch
"""

print("="*60)
print("–®–ê–ì 3: –ü–û–ò–°–ö–û–í–ê–Ø –°–ò–°–¢–ï–ú–ê –ü–û –°–ú–´–°–õ–£")
print("="*60)

# –ó–ê–ì–†–£–ñ–ê–ï–ú –î–ê–ù–ù–´–ï –ò–ó –ü–†–ï–î–´–î–£–©–ï–ì–û –®–ê–ì–ê
import pickle
import numpy as np
import matplotlib.pyplot as plt

try:
    with open('rag_data_step2.pkl', 'rb') as f:
        rag_data = pickle.load(f)

    print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞ 'rag_data_step2.pkl'")

    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    chunks = rag_data["chunks"]
    chunk_info = rag_data["chunk_info"]
    embeddings = rag_data["embeddings"]

    # –ú–æ–¥–µ–ª—å —É–∂–µ –Ω–µ –Ω—É–∂–Ω–∞ - —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã
    print(f"–í –±–∞–∑–µ: {len(chunks)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    print(f"–†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –ø–æ–∏—Å–∫–∞: {embeddings.shape[1]} —á–∏—Å–µ–ª")

except FileNotFoundError:
    print("‚ùå –§–∞–π–ª 'rag_data_step2.pkl' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ 2_embedding_visual.py")
    exit()

print("üöÄ –ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞
def calculate_similarity(vec1, vec2):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)

    if norm1 == 0 or norm2 == 0:
        return 0

    similarity = dot_product / (norm1 * norm2)
    return similarity

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
def semantic_search(query, top_k=3):
    """
    –ò—â–µ—Ç —Å–∞–º—ã–µ –ø–æ—Ö–æ–∂–∏–µ —á–∞–Ω–∫–∏ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """

    print(f"\nüîé –ü–û–ò–°–ö: '{query}'")
    print("-" * 50)

    # 1. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
    print("1Ô∏è‚É£ –ü—Ä–µ–æ–±—Ä–∞–∑—É—é –≤–æ–ø—Ä–æ—Å –≤ –≤–µ–∫—Ç–æ—Ä...")

    # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–∏–π –≤–µ–∫—Ç–æ—Ä –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤
    # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã –º–æ–¥–µ–ª—å, –Ω–æ –¥–ª—è –¥–µ–º–æ —Ç–∞–∫
    query_words = query.lower().split()

    # –ù–∞—Ö–æ–¥–∏–º –≤–µ–∫—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–∞ –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —ç—Ç–∏ —Å–ª–æ–≤–∞
    query_embedding = np.zeros(embeddings.shape[1])
    matching_chunks = 0

    for i, chunk in enumerate(chunks):
        chunk_lower = chunk.lower()
        if any(word in chunk_lower for word in query_words):
            query_embedding += embeddings[i]
            matching_chunks += 1

    if matching_chunks > 0:
        query_embedding /= matching_chunks
    else:
        # –ï—Å–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä
        query_embedding = embeddings[0].copy()

    print(f"   –í–æ–ø—Ä–æ—Å ‚Üí –≤–µ–∫—Ç–æ—Ä –∏–∑ {len(query_embedding)} —á–∏—Å–µ–ª")

    # 2. –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ –≤—Å–µ–º–∏ —á–∞–Ω–∫–∞–º–∏
    print(f"\n2Ô∏è‚É£ –°—Ä–∞–≤–Ω–∏–≤–∞—é —Å {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏...")

    similarities = []
    for i, chunk_embedding in enumerate(embeddings):
        similarity = calculate_similarity(query_embedding, chunk_embedding)
        similarities.append((i, similarity))

    # 3. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
    similarities.sort(key=lambda x: x[1], reverse=True)

    # 4. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\n3Ô∏è‚É£ –ù–∞–π–¥–µ–Ω–æ {len(similarities)} –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π")
    print(f"   –í—ã–±–∏—Ä–∞—é {top_k} —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö:")

    results = []
    for rank, (idx, similarity) in enumerate(similarities[:top_k], 1):
        print(f"\n   #{rank} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.3f}):")
        print(f"   üìÑ {chunks[idx][:80]}...")

        results.append({
            "rank": rank,
            "chunk_index": idx,
            "similarity": similarity,
            "text": chunks[idx],
            "doc_id": chunk_info[idx]["doc_id"]
        })

    return results, query_embedding, similarities

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞
def visualize_search(query_embedding, search_results):
    """
    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –ö–ê–ö —Ä–∞–±–æ—Ç–∞–ª –ø–æ–∏—Å–∫ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ
    """
    print("\n" + "="*60)
    print("üìä –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ü–†–û–¶–ï–°–°–ê –ü–û–ò–°–ö–ê")
    print("="*60)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º PCA –∏–∑ scikit-learn (–¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ torch)
    try:
        from sklearn.decomposition import PCA

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–∞ –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º –¥–ª—è PCA
        all_embeddings = np.vstack([embeddings, query_embedding.reshape(1, -1)])

        pca = PCA(n_components=2)
        all_embeddings_2d = pca.fit_transform(all_embeddings)

        # –ü–æ—Å–ª–µ–¥–Ω—è—è —Ç–æ—á–∫–∞ - —ç—Ç–æ –Ω–∞—à –≤–æ–ø—Ä–æ—Å
        question_2d = all_embeddings_2d[-1]
        chunks_2d = all_embeddings_2d[:-1]

        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(12, 9))

        # 1. –†–∏—Å—É–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ (—Å–µ—Ä—ã–º)
        plt.scatter(chunks_2d[:, 0], chunks_2d[:, 1],
                    color='lightgray', s=50, alpha=0.5, label='–í—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã')

        # 2. –ü–æ–¥—Å–≤–µ—á–∏–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        colors = ['red', 'orange', 'green']
        for i, result in enumerate(search_results):
            idx = result["chunk_index"]
            x, y = chunks_2d[idx]

            # –¢–æ—á–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            plt.scatter(x, y, color=colors[i], s=300,
                       alpha=0.8, label=f'–†–µ–∑—É–ª—å—Ç–∞—Ç #{i+1}')

            # –õ–∏–Ω–∏—è –æ—Ç –≤–æ–ø—Ä–æ—Å–∞ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
            plt.plot([question_2d[0], x], [question_2d[1], y],
                    color=colors[i], alpha=0.5, linestyle='--')

            # –ü–æ–¥–ø–∏—Å—å —Å —Ä–µ–π—Ç–∏–Ω–≥–æ–º
            plt.text(x, y, f' #{i+1}\n({result["similarity"]:.2f})',
                    fontsize=10, ha='center', va='center',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

        # 3. –í–µ–∫—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–∞ (–±–æ–ª—å—à–∞—è –∑–≤–µ–∑–¥–∞)
        plt.scatter(question_2d[0], question_2d[1],
                    color='blue', s=500, marker='*',
                    label='–í–∞—à –≤–æ–ø—Ä–æ—Å', edgecolors='black', linewidth=2)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥—Ä–∞—Ñ–∏–∫–∞
        plt.title('–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫', fontsize=16, pad=20)
        plt.xlabel('–ì–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 1', fontsize=12)
        plt.ylabel('–ì–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 2', fontsize=12)
        plt.legend(loc='upper right')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        plt.savefig('search_visualization.png', dpi=150, bbox_inches='tight')
        print("‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'search_visualization.png'")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.show()

    except ImportError:
        print("‚ö†Ô∏è  scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn")

# –¢–ï–°–¢–ò–†–£–ï–ú –ü–û–ò–°–ö–û–í–£–Æ –°–ò–°–¢–ï–ú–£
print("\n" + "="*60)
print("–¢–ï–°–¢–ò–†–£–ï–ú –ü–û–ò–°–ö–û–í–£–Æ –°–ò–°–¢–ï–ú–£")
print("="*60)

# –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤
test_questions = [
    "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
    "–û–±—ä—è—Å–Ω–∏ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
    "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ NLP?"
]

print("\n–î–∞–≤–∞–π—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫!")
print("–í–æ–ø—Ä–æ—Å: '–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?'")

# –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
results, query_embedding, all_similarities = semantic_search(test_questions[0])

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º
visualize_search(query_embedding, results)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
rag_data["search_results"] = results
rag_data["query_embedding"] = query_embedding

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
with open('rag_data_step3.pkl', 'wb') as f:
    pickle.dump(rag_data, f)

print("\n" + "="*60)
print("–ò–¢–û–ì –≠–¢–ê–ü–ê 3:")
print("="*60)
print("‚úÖ –°–æ–∑–¥–∞–ª–∏ –ø–æ–∏—Å–∫–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ —Å–º—ã—Å–ª—É")
print("‚úÖ –ù–∞—É—á–∏–ª–∏ –µ–µ –∏—Å–∫–∞—Ç—å –±–µ–∑ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å–ª–æ–≤")
print("‚úÖ –î–æ–±–∞–≤–∏–ª–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–æ–∏—Å–∫–∞")
print("üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'rag_data_step3.pkl'")
print("\n–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å: python 4_rag_pipeline.py")